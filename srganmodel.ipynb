{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "srganmodel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkdErFlMsd1sh1hy7fZ4DW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itzThillaiC/AI-Fitness-trainer/blob/main/srganmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TXQsZPrtXjc0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = [\n",
        "    \"ResidualConvBlock\",\n",
        "    \"Discriminator\", \"Generator\",\n",
        "    \"ContentLoss\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "7EUj8K3UYYqv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConvBlock(nn.Module):\n",
        "    \"\"\"Implements residual conv function.\n",
        "    Args:\n",
        "        channels (int): Number of channels in the input image.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super(ResidualConvBlock, self).__init__()\n",
        "        self.rcb = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.rcb(x)\n",
        "\n",
        "        out = torch.add(out, identity)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "qYrCeyn0YeN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        self.upsample_block = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels * 4, (3, 3), (1, 1), (1, 1)),\n",
        "            nn.PixelShuffle(2),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        out = self.upsample_block(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ZcTZj50GYktO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # input size. (3) x 96 x 96\n",
        "            nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (64) x 48 x 48\n",
        "            nn.Conv2d(64, 64, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(64, 128, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (128) x 24 x 24\n",
        "            nn.Conv2d(128, 128, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(128, 256, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (256) x 12 x 12\n",
        "            nn.Conv2d(256, 256, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(256, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            # state size. (512) x 6 x 6\n",
        "            nn.Conv2d(512, 512, (3, 3), (2, 2), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 6 * 6, 1024),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1024, 1),\n",
        "        )\n",
        "\n",
        "        def forward(self, x: Tensor) -> Tensor:\n",
        "        out = self.features(x)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "ykprh6x5ZBZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(Generator, self).__init__()\n",
        "        # First conv layer.\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, (9, 9), (1, 1), (4, 4)),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "\n",
        "        # Features trunk blocks.\n",
        "        trunk = []\n",
        "        for _ in range(16):\n",
        "            trunk.append(ResidualConvBlock(64))\n",
        "        self.trunk = nn.Sequential(*trunk)\n",
        "\n",
        "        # Second conv layer.\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1), bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "        )\n",
        "\n",
        "        # Upscale block\n",
        "        upsampling = []\n",
        "        for _ in range(2):\n",
        "            upsampling.append(UpsampleBlock(64))\n",
        "        self.upsampling = nn.Sequential(*upsampling)\n",
        "\n",
        "        # Output layer.\n",
        "        self.conv_block3 = nn.Conv2d(64, 3, (9, 9), (1, 1), (4, 4))\n",
        "\n",
        "        # Initialize neural network weights.\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "    # Support torch.script function.\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        out1 = self.conv_block1(x)\n",
        "        out = self.trunk(out1)\n",
        "        out2 = self.conv_block2(out)\n",
        "        out = torch.add(out1, out2)\n",
        "        out = self.upsampling(out)\n",
        "        out = self.conv_block3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _initialize_weights(self) -> None:\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.BatchNorm2d):\n",
        "                nn.init.constant_(module.weight, 1)"
      ],
      "metadata": {
        "id": "CYvg2MJIax5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentLoss(nn.Module):\n",
        "    \"\"\"Constructs a content loss function based on the VGG19 network.\n",
        "    Using high-level feature mapping layers from the latter layers will focus more on the texture content of the image.\n",
        "    Paper reference list:\n",
        "        -`Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network <https://arxiv.org/pdf/1609.04802.pdf>` paper.\n",
        "        -`ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks                    <https://arxiv.org/pdf/1809.00219.pdf>` paper.\n",
        "        -`Perceptual Extreme Super Resolution Network with Receptive Field Block               <https://arxiv.org/pdf/2005.12597.pdf>` paper.\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # Load the VGG19 model trained on the ImageNet dataset.\n",
        "        vgg19 = models.vgg19(pretrained=True).eval()\n",
        "        # Extract the thirty-sixth layer output in the VGG19 model as the content loss.\n",
        "        self.feature_extractor = nn.Sequential(*list(vgg19.features.children())[:36])\n",
        "        # Freeze model parameters.\n",
        "        for parameters in self.feature_extractor.parameters():\n",
        "            parameters.requires_grad = False\n",
        "\n",
        "        # The preprocessing method of the input data. This is the VGG model preprocessing method of the ImageNet dataset.\n",
        "        self.register_buffer(\"mean\", torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, sr: Tensor, hr: Tensor) -> Tensor:\n",
        "        # Standardized operations\n",
        "        sr = sr.sub(self.mean).div(self.std)\n",
        "        hr = hr.sub(self.mean).div(self.std)\n",
        "\n",
        "        # Find the feature map difference between the two images\n",
        "        loss = F.l1_loss(self.feature_extractor(sr), self.feature_extractor(hr))\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "QP2m-0YaeLS3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}